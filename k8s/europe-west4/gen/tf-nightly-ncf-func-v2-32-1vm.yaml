# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"apiVersion": "batch/v1beta1"
"kind": "CronJob"
"metadata":
  "labels":
    "accelerator": "v2-32"
    "benchmarkId": "tf-nightly-ncf-func-v2-32-1vm"
    "frameworkVersion": "tf-nightly"
    "mode": "func"
    "model": "ncf"
  "name": "tf-nightly-ncf-func-v2-32-1vm"
  "namespace": "automated"
"spec":
  "concurrencyPolicy": "Forbid"
  "jobTemplate":
    "metadata":
      "annotations":
        "ml-testing-accelerators/gcs-subdir": "tf-nightly/ncf/func/v2-32"
        "ml-testing-accelerators/metric-config": |
          {
            "sources": [
              {
                "literals": {
                  "assertions": {
                    "duration": {
                      "inclusive_bounds": false,
                      "std_devs_from_mean": {
                        "comparison": "LESS",
                        "std_devs": 5
                      },
                      "wait_for_n_data_points": 10
                    }
                  }
                }
              },
              {
                "tensorboard": {
                  "aggregate_assertions": [
                    {
                      "assertion": {
                        "inclusive_bounds": true,
                        "std_devs_from_mean": {
                          "comparison": "GREATER",
                          "std_devs": 4
                        },
                        "wait_for_n_data_points": 0
                      },
                      "strategy": "AVERAGE",
                      "tag": "examples_per_second"
                    }
                  ],
                  "exclude_tags": [
          
                  ],
                  "include_tags": [
                    {
                      "strategies": [
                        "FINAL"
                      ],
                      "tag_pattern": "*"
                    }
                  ],
                  "merge_runs": false
                }
              }
            ]
          }
      "labels":
        "accelerator": "v2-32"
        "benchmarkId": "tf-nightly-ncf-func-v2-32-1vm"
        "frameworkVersion": "tf-nightly"
        "mode": "func"
        "model": "ncf"
    "spec":
      "activeDeadlineSeconds": 3600
      "backoffLimit": 1
      "template":
        "metadata":
          "annotations":
            "reserved.cloud-tpus.google.com": "false"
            "tf-version.cloud-tpus.google.com": "v2-nightly-pod"
        "spec":
          "containers":
          - "args":
            - "python3"
            - "official/recommendation/ncf_keras_main.py"
            - "--train_dataset_path=$(NCF_DIR)/tpu_data_dir/training_cycle_*/*"
            - "--eval_dataset_path=$(NCF_DIR)/tpu_data_dir/eval_data/*"
            - "--input_meta_data_path=$(NCF_DIR)/tpu_data_dir/metadata"
            - "--data_dir=$(NCF_DIR)/movielens_data"
            - "--batch_size=99000"
            - "--learning_rate=3e-5"
            - "--dataset=ml-20m"
            - "--eval_batch_size=160000"
            - "--learning_rate=0.00382059"
            - "--beta1=0.783529"
            - "--beta2=0.909003"
            - "--epsilon=1.45439e-07"
            - "--num_factors=64"
            - "--hr_threshold=0.635"
            - "--layers=256,256,128,64"
            - "--use_synthetic_data=false"
            - "--model_dir=$(MODEL_DIR)"
            - "--train_epochs=1"
            - "--ml_perf=true"
            - "--keras_use_ctl=true"
            "command":
            - "bash"
            - "-c"
            - |
              set -x
              set -u
              ssh -i scripts/id_rsa -o StrictHostKeyChecking=no xl-ml-test@$(cat /scripts/tpu_ip) \
                'gcloud auth configure-docker'
              ssh -i scripts/id_rsa -o StrictHostKeyChecking=no xl-ml-test@$(cat /scripts/tpu_ip) \
                'sudo gcsfuse --implicit-dirs -o allow_other /gcs'
              ssh -i scripts/id_rsa -o StrictHostKeyChecking=no xl-ml-test@$(cat /scripts/tpu_ip) \
                'sudo docker run -i --rm --privileged -v "/gcs:/gcs" -v "$(LOCAL_OUTPUT_DIR):$(LOCAL_OUTPUT_DIR)" --entrypoint "" -v "/lib/libtpu.so:/lib/libtpu.so" --net host -e TPU_LOAD_LIBRARY=0 gcr.io/xl-ml-test/tensorflow-1vm:nightly ''"python3" "official/recommendation/ncf_keras_main.py" "--train_dataset_path=$(NCF_DIR)/tpu_data_dir/training_cycle_*/*" "--eval_dataset_path=$(NCF_DIR)/tpu_data_dir/eval_data/*" "--input_meta_data_path=$(NCF_DIR)/tpu_data_dir/metadata" "--data_dir=$(NCF_DIR)/movielens_data" "--batch_size=99000" "--learning_rate=3e-5" "--dataset=ml-20m" "--eval_batch_size=160000" "--learning_rate=0.00382059" "--beta1=0.783529" "--beta2=0.909003" "--epsilon=1.45439e-07" "--num_factors=64" "--hr_threshold=0.635" "--layers=256,256,128,64" "--use_synthetic_data=false" "--model_dir=$(MODEL_DIR)" "--train_epochs=1" "--ml_perf=true" "--keras_use_ctl=true"'
              exit_code=$?
              ssh -i scripts/id_rsa -o StrictHostKeyChecking=no xl-ml-test@$(cat /scripts/tpu_ip) 'gsutil -m cp -r $(LOCAL_OUTPUT_DIR) $(MODEL_DIR)'
              bash /scripts/cleanup.sh
              exit $exit_code
            "env":
            - "name": "POD_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.name"
            - "name": "POD_UID"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.uid"
            - "name": "POD_NAMESPACE"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.namespace"
            - "name": "JOB_NAME"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.labels['job-name']"
            - "name": "MODEL_DIR"
              "value": "$(OUTPUT_BUCKET)/tf-nightly/ncf/func/v2-32/$(JOB_NAME)"
            - "name": "KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS"
              "value": "tpu-$(POD_UID)"
            - "name": "LOCAL_OUTPUT_DIR"
              "value": "/tmp/model_dir"
            "envFrom":
            - "configMapRef":
                "name": "gcs-buckets"
            "image": "google/cloud-sdk"
            "imagePullPolicy": "Always"
            "lifecycle":
              "preStop":
                "exec":
                  "command":
                  - "bash"
                  - "/scripts/cleanup.sh"
            "name": "train"
            "resources":
              "limits":
                "tpu.googleapis.com/v2": 32
              "requests":
                "cpu": 2
                "memory": "20G"
            "volumeMounts":
            - "mountPath": "/scripts"
              "name": "scripts"
              "readOnly": false
            - "mountPath": "/dev/shm"
              "name": "dshm"
              "readOnly": false
          "initContainers":
          - "command":
            - "/bin/bash"
            - "-c"
            - |
              set -u
              set -e
              set -x
              
              project=$(curl -sS "http://metadata.google.internal/computeMetadata/v1/project/project-id" -H "Metadata-Flavor: Google")
              zone=$(curl -sS "http://metadata.google.internal/computeMetadata/v1/instance/zone" -H "Metadata-Flavor: Google" | awk -F'/' '{print $4}')
              tpu_name=tpu-${POD_UID}
              ssh-keygen -t rsa -f /scripts/id_rsa -q -N ""
              
              echo "
              curl -X DELETE \
                -H \"Authorization: Bearer \$(gcloud auth print-access-token)\" \
                https://tpu.googleapis.com/v2alpha1/projects/${project}/locations/${zone}/nodes/${tpu_name}
              sleep 60
              " > /scripts/cleanup.sh
              
              curl -X POST \
                -H "Authorization: Bearer $(gcloud auth print-access-token)" \
                -H "Content-Type: application/json" \
                -d "{
                  accelerator_type: 'v2-32',
                  runtime_version: 'v2-nightly-pod',
                  network_config: {enable_external_ips: true},
                  metadata: {
                    'ssh-keys': 'xl-ml-test:$(cat /scripts/id_rsa.pub)',
                    'startup-script': 'echo Running startup script'
                  }
                }" https://tpu.googleapis.com/v2alpha1/projects/${project}/locations/${zone}/nodes?node_id=${tpu_name}
              
              echo "Waiting for TPU Pod ${tpu_name} to become ready..."
              while [[ ${health:-NONE} != "READY" ]];
                do sleep 10 && \
                health=$(gcloud \
                  --project=${project} \
                  compute \
                  tpus \
                  describe \
                  ${tpu_name} \
                  --zone=${zone} \
                  --format="value(state)") && \
                echo "Waiting for ready TPU (current state ${health:-NONE})...";
              done
              
              echo ${tpu_name} > /scripts/tpu_name
              gcloud compute tpus describe ${tpu_name} --project=${project} --zone=${zone} --format="value(ipAddress)" > /scripts/tpu_ip
              
              sleep 60
              
            "env":
            - "name": "POD_UID"
              "valueFrom":
                "fieldRef":
                  "fieldPath": "metadata.uid"
            "image": "google/cloud-sdk"
            "name": "create-tpu"
            "volumeMounts":
            - "mountPath": "/scripts"
              "name": "scripts"
          "nodeSelector":
            "tpu-available": "true"
          "restartPolicy": "Never"
          "volumes":
          - "emptyDir":
              "medium": "Memory"
            "name": "scripts"
          - "emptyDir":
              "medium": "Memory"
            "name": "dshm"
  "schedule": "30 13 * * *"
  "successfulJobsHistoryLimit": 1